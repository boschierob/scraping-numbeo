# Numbeo Scraping Project - Modular Structure

Ce projet a Ã©tÃ© refactorisÃ© en une structure modulaire pour amÃ©liorer la maintenabilitÃ© et l'extensibilitÃ©.

## Structure du Projet

```
Numbeo-scraping/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py          # Configuration globale
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ city_loader.py       # Chargement des donnÃ©es des villes
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ url_builder.py       # Construction des URLs
â”‚   â”‚   â””â”€â”€ file_saver.py        # Sauvegarde des fichiers
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ base_scraper.py      # Classe de base pour les scrapers
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ stats_tracker.py     # Suivi des statistiques
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ main.py                      # Point d'entrÃ©e principal
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Modules

### 1. Configuration (`src/config/`)
- **settings.py** : Tous les paramÃ¨tres de configuration (URLs, dÃ©lais, sÃ©lecteurs, etc.)

### 2. DonnÃ©es (`src/data/`)
- **city_loader.py** : Chargement et validation des donnÃ©es des villes depuis le CSV

### 3. Utilitaires (`src/utils/`)
- **url_builder.py** : Construction des URLs pour les diffÃ©rentes catÃ©gories
- **file_saver.py** : Sauvegarde des donnÃ©es en CSV et Excel

### 4. Scrapers (`src/scrapers/`)
- **base_scraper.py** : Classe de base avec fonctionnalitÃ©s communes (requÃªtes HTTP, parsing HTML, etc.)

### 5. Monitoring (`src/monitoring/`)
- **stats_tracker.py** : Suivi des statistiques et gÃ©nÃ©ration de rapports

## Utilisation

### Installation
```bash
pip install -r requirements.txt
```

### ExÃ©cution
```bash
python main.py
```

## FonctionnalitÃ©s ImplÃ©mentÃ©es

### âœ… Structure de base modulaire
- SÃ©paration claire des responsabilitÃ©s
- Configuration centralisÃ©e
- Gestion des erreurs et logging
- Suivi des statistiques

### âœ… Chargement des donnÃ©es
- Lecture du fichier CSV des villes
- Validation des URLs
- Gestion des erreurs de chargement

### âœ… Construction des URLs
- GÃ©nÃ©ration automatique des URLs pour toutes les catÃ©gories
- Validation des URLs
- Extraction des identifiants de ville

### âœ… Sauvegarde des fichiers
- Sauvegarde en CSV et Excel
- Organisation par catÃ©gories
- Noms de fichiers sÃ©curisÃ©s
- Gestion des dossiers de sortie

### âœ… Monitoring et rapports
- Suivi dÃ©taillÃ© des statistiques
- GÃ©nÃ©ration de rapports JSON et texte
- Calcul des taux de succÃ¨s
- Gestion des erreurs

## Prochaines Ã‰tapes

### ğŸ”„ Ã€ implÃ©menter
1. **Scrapers spÃ©cifiques par catÃ©gorie** :
   - Quality of Life scraper
   - Crime scraper
   - Cost of Living scraper
   - Health Care scraper
   - Climate scraper
   - Property Investment scraper
   - Traffic scraper
   - Pollution scraper

2. **IntÃ©gration du scraping rÃ©el** dans `main.py`

3. **Script d'import MySQL** conditionnel

4. **Tests unitaires** pour chaque module

## Avantages de cette Structure

1. **MaintenabilitÃ©** : Code organisÃ© et facile Ã  maintenir
2. **ExtensibilitÃ©** : Facile d'ajouter de nouvelles catÃ©gories ou fonctionnalitÃ©s
3. **TestabilitÃ©** : Chaque module peut Ãªtre testÃ© indÃ©pendamment
4. **RÃ©utilisabilitÃ©** : Composants rÃ©utilisables dans d'autres projets
5. **DÃ©bogage** : Logging et monitoring dÃ©taillÃ©s

## Configuration

Tous les paramÃ¨tres sont centralisÃ©s dans `src/config/settings.py` :
- URLs de base
- DÃ©lais entre requÃªtes
- SÃ©lecteurs de tables
- ParamÃ¨tres de base de donnÃ©es
- Configuration du logging

## Logs et Rapports

Le systÃ¨me gÃ©nÃ¨re automatiquement :
- Logs dÃ©taillÃ©s dans `logs/scraping.log`
- Rapports de session dans le dossier de sortie
- Statistiques de succÃ¨s/Ã©chec
- Liste des erreurs rencontrÃ©es 